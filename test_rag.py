import os
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 1. SETUP
load_dotenv()
CHROMA_PATH = "./chroma_db"

PROMPT_TEMPLATE = """
Tu es un assistant expert en droits administratifs pour les personnes handicap√©es.
R√©ponds √† la question en t'basant UNIQUEMENT sur le contexte suivant :

{context}

---
Question : {question}
"""

def query_rag(query_text: str):
    # 2. PR√âPARATION DU LLM & DB
    embedding_function = OpenAIEmbeddings()
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # 3. RECHERCHE (RETRIEVAL)
    # On cherche les 3 morceaux (chunks) les plus proches s√©mantiquement
    print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è Recherche dans la base pour : '{query_text}'...")
    results = db.similarity_search(query_text, k=3)

    if len(results) == 0:
        print("‚ùå Aucune correspondance trouv√©e.")
        return

    # On colle les morceaux de texte ensemble
    context_text = "\n\n---\n\n".join([doc.page_content for doc in results])
    
    # 4. G√âN√âRATION (GENERATION)
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context_text, question=query_text)

    print("ü§ñ G√©n√©ration de la r√©ponse via GPT-4o...")
    model = ChatOpenAI(model="gpt-4o-mini") # On utilise le mini pour √©conomiser tes sous
    response = model.invoke(prompt)

    # 5. R√âSULTAT
    print("\n" + "="*50)
    print(f"üì¢ R√âPONSE : \n{response.content}")
    print("="*50)
    
    # Bonus : Afficher les sources pour v√©rifier qu'il n'invente pas
    print("\nüìö SOURCES UTILIS√âES :")
    for doc in results:
        print(f"- {doc.metadata.get('source', 'Inconnu')} (Page {doc.metadata.get('page', '?')})")

if __name__ == "__main__":
    # Change cette question pour tester diff√©rents sujets !
    ma_question = "Quelles sont les conditions pour toucher l'AAH ?"
    query_rag(ma_question)