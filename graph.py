import os
from dotenv import load_dotenv
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# Import de ton outil RAG
from rag_tool import retrieve_context

load_dotenv()

# 1. D√âFINITION DE LA M√âMOIRE (STATE)
# C'est ici qu'on stocke toute la conversation
class State(TypedDict):
    # "add_messages" permet d'ajouter les nouveaux messages √† la liste existante (append)
    messages: Annotated[list, add_messages]

# ... (Tes imports restent pareils)

def chatbot_node(state: State):
    llm = ChatOpenAI(model="gpt-4o-mini")
    
    # On r√©cup√®re le dernier message
    last_message = state["messages"][-1].content
    
    # On r√©cup√®re le profil utilisateur s'il existe (sinon d√©faut)
    # LangGraph stocke tout dans le state, on va tricher un peu pour le MVP
    # et l'injecter via le SystemPrompt direct.
    
    context = retrieve_context(last_message)
    
    # üö® LA NOUVEAUT√â EST ICI : LE PROMPT DYNAMIQUE
    # On d√©finit comment l'IA doit parler selon le profil choisi dans l'UI
    # (Note: Dans une version V2, ce serait pass√© proprement dans le state)
    user_profile = "Standard" 
    # Petite astuce : on va choper le profil depuis le dernier message humain 
    # s'il contient une m√©ta-data (hack MVP) ou on le laisse g√©n√©rique.
    # Pour l'instant, on va g√©rer √ßa dans le Prompt Template directement.

    prompt = f"""
    Tu es l'assistant One For All.
    
    CONTEXTE DOCUMENTAIRE :
    {context}
    
    CONSIGNES D'ADAPTATION :
    - Si l'utilisateur demande du FALC (Facile √† Lire), fais des phrases courtes, mots simples, listes √† puces.
    - Si l'utilisateur a un TDAH, mets en GRAS les mots cl√©s importants et sois ultra-concis.
    - Sinon, r√©ponds normalement mais poliment.
    
    R√©ponds √† la question en utilisant le contexte. Si tu ne sais pas, dis-le.
    """
    
    messages = [SystemMessage(content=prompt)] + state["messages"]
    response = llm.invoke(messages)
    
    return {"messages": [response]}

# 3. CONSTRUCTION DU GRAPHE
workflow = StateGraph(State)

# On ajoute notre noeud unique
workflow.add_node("chatbot", chatbot_node)

# On d√©finit le flux : D√©but -> Chatbot -> Fin
workflow.add_edge(START, "chatbot")
workflow.add_edge("chatbot", END)

# On compile le cerveau
app = workflow.compile()

# ==========================================
# TEST RAPIDE (Simulation de conversation)
# ==========================================
if __name__ == "__main__":
    print("üß† D√©marrage du Cerveau LangGraph...")
    
    # Simulation d'un utilisateur qui pose 2 questions √† la suite
    inputs = {"messages": [HumanMessage(content="Quelles sont les conditions de l'AAH ?")]}
    
    # 1√®re r√©ponse
    print("\n--- Tour 1 ---")
    for event in app.stream(inputs):
        for value in event.values():
            print("ü§ñ Agent:", value["messages"][-1].content)
            
    # On simule la m√©moire en gardant l'√©tat (dans la vraie vie, l'interface g√©rera √ßa)
    # Pour ce test simple, on lance juste une 2√®me question ind√©pendante pour v√©rifier que √ßa ne plante pas
    print("\n--- Tour 2 ---")
    inputs2 = {"messages": [HumanMessage(content="Et pour la MDPH, je fais comment ?")]}
    for event in app.stream(inputs2):
        for value in event.values():
            print("ü§ñ Agent:", value["messages"][-1].content)